% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/deepregression.R
\name{deepregression}
\alias{deepregression}
\title{Fitting Deep Distributional Regression}
\usage{
deepregression(y, list_of_formulae, list_of_deep_models,
  family = c("normal", "bernoulli", "bernoulli_prob", "beta", "betar",
  "cauchy", "chi2", "chi", "exponential", "gamma_gamma", "gamma", "gammar",
  "gumbel", "half_cauchy", "half_normal", "horseshoe", "inverse_gamma",
  "inverse_gaussian", "laplace", "log_normal", "logistic", "negbinom",
  "pareto", "poisson", "poisson_lograte", "student_t", "student_t_ls",
  "truncated_normal", "uniform"), train_together = FALSE,
  mean_regression = FALSE, data, df = NULL, lambda_lasso = NULL,
  defaultSmoothing = NULL, cv_folds = NULL, validation_data = NULL,
  validation_split = ifelse(is.null(validation_data) & is.null(cv_folds),
  0.2, 0), dist_fun = NULL, learning_rate = 0.01,
  optimizer = optimizer_adam(lr = learning_rate), variational = FALSE,
  monitor_metric = list(), seed = 1991 - 5 - 4, ...)
}
\arguments{
\item{y}{response variable}

\item{list_of_formulae}{a named list of right hand side formulae,
one for each parameter of the distribution specified in \code{family};
set to \code{~ 1} if the parameter should be treated as constant.
Use the \code{s()}-notation from \code{mgcv} for specification of
non-linear structured effects and \code{d(..., model = ...)} for
deep learning predictors (separated by commas),
where \code{model} is the an integer
giving the index of the deep model in \code{list_of_deep_models} to
be used for the predictors}

\item{list_of_deep_models}{a list of (lists of) functions
specifying a keras model for each parameter of interest.
See the examples for more details.}

\item{family}{a character specifying the distribution. For information on 
possible distribution and parameters, see \code{\link{make_tfd_dist}}}

\item{train_together}{logical; whether or not to train all parameters in 
one deep network.}

\item{df}{degrees of freedom for all non-linear structural terms}

\item{lambda_lasso}{smoothing parameter for lasso regression}

\item{defaultSmoothing}{function applied to all s-terms, per default (NULL)
the minimum df of all possible terms is used.}

\item{cv_folds}{a list of lists, each list element has two elements, one for
training indices and one for testing indices; if a single integer number is given, 
a simple k-fold cross-validation is defined, where k is the supplied number.}

\item{validation_data}{data for validation during training.}

\item{dist_fun}{a custom distribution applied to the last layer,
see \code{\link{make_tfd_dist}} for more details on how to construct
a custom distribution function.}

\item{learning_rate}{learning rate for optimizer}

\item{optimizer}{optimzer used. Per default ADAM.}

\item{variational}{logical value specifying whether or not to use
variational inference. If \code{TRUE}, details must be passed to
the via the ellipsis to the initialization function
(see \code{\link{deepregression_init}})}

\item{monitor_metric}{Further metrics to monitor}

\item{...}{further arguments passed to the \code{deepRegression_init} function}

\item{validation_spit}{percentage of training data used for validation. Per default 0.2.}
}
\description{
Fitting Deep Distributional Regression
}
\examples{
library(deepregression)

data = data.frame(matrix(rnorm(10*100), c(100,10)))
colnames(data) <- c("x1","x2","x3","xa","xb","xc","xd","xe","xf","unused")
formula <- ~ 1 + d(x1,x2,x3) +
s(xa, sp = 1) + te(xe,xf) + x1

deep_model <- function(x) x \%>\%
layer_dense(units = 128, activation = "relu", use_bias = FALSE) \%>\%
layer_dense(units = 64, activation = "relu") \%>\%
layer_dropout(rate = 0.2) \%>\%
layer_dense(units = 32, activation = "relu") \%>\%
layer_dropout(rate = 0.2) \%>\%
layer_dense(units = 8, activation = "relu") \%>\%
layer_dense(units = 1, activation = "linear")

y <- rnorm(100)

mod <- deepregression(list_of_formulae = list(loc = formula, scale = ~ 1),
data = data, validation_data = list(data, y), y = y,
list_of_deep_models = list(deep_model, NULL))

mod \%>\% fit(epochs = 100)
mod \%>\% plot()
}
